# RAG ChatBot â€” Hybrid General + RAG Chat

A **production-ready** RAG (Retrieval-Augmented Generation) chatbot with a polished Next.js frontend and a FastAPI backend powered by **Gemini 2.5 Flash** and **ChromaDB**. Supports hybrid chat: general AI conversation when no document is loaded, and document-grounded RAG when a file is uploaded and the toggle is ON.

[![Deploy with Vercel](https://vercel.com/button)](https://vercel.com/new/clone?repository-url=https://github.com/Otachiking/Chatbot-RAG&root-directory=frontend)
[![Deploy to Render](https://render.com/images/deploy-to-render-button.svg)](https://render.com/deploy)

## Author

**Muhammad Iqbal Rasyid**  
ğŸŒ [Portfolio](https://portfolio-otachiking.vercel.app/)  
ğŸ“§ GitHub: [@Otachiking](https://github.com/Otachiking)

## Features

- **Hybrid chat mode** â€” General AI chat by default; RAG mode activates after file upload.
- **Explicit RAG toggle** â€” "Use document reference" switch lets users control when retrieval is used.
- **PDF + image upload** â€” Drag-and-drop support for `.pdf`, `.png`, `.jpg`, `.jpeg`.
- **Image OCR** â€” pytesseract extracts text from images, treated as single-page documents.
- **PDF text extraction** â€” PyMuPDF with OCR fallback for scanned pages.
- **Vector search** â€” ChromaDB stores chunks with Gemini embeddings, filtered by `file_id`.
- **Citation badges** â€” Bot answers include `Hal. X` citation badges linking to source pages.
- **Typing indicator** â€” 3-dot bounce animation during LLM generation.
- **Recommend actions** â€” "Summarize" and "Generate Quiz" buttons after upload.
- **Export transcript** â€” Download full conversation as `.txt`.
- **Filename overflow fix** â€” Long filenames scroll horizontally within their container.
- **Toast notifications** â€” Error, success, and info feedback.
- **Query logging** â€” Every query logged as JSON lines with latency metrics.
- **Metrics report** â€” CLI tool to compute average latency and precision@K.

## Repository structure

```
ChatbotRAG/
â”œâ”€ frontend/                        # Next.js (React + TypeScript)
â”‚  â”œâ”€ package.json
â”‚  â”œâ”€ pages/
â”‚  â”‚  â”œâ”€ _app.tsx
â”‚  â”‚  â””â”€ index.tsx                  # Main page â€” state, layout, hybrid logic
â”‚  â”œâ”€ components/
â”‚  â”‚  â”œâ”€ ChatWindow.tsx             # Chat bubbles, typing indicator, export
â”‚  â”‚  â”œâ”€ FileUploader.tsx           # Drag & drop, progress, image/PDF preview
â”‚  â”‚  â”œâ”€ RecommendCard.tsx          # Summarize / Generate Quiz
â”‚  â”‚  â””â”€ RagToggle.tsx              # "Use document reference" toggle
â”‚  â””â”€ styles/globals.css
â”œâ”€ backend/
â”‚  â”œâ”€ .env                          # API keys (NEVER commit)
â”‚  â”œâ”€ requirements.txt
â”‚  â”œâ”€ main.py                       # FastAPI app â€” mounts routers
â”‚  â”œâ”€ routers/
â”‚  â”‚  â”œâ”€ upload.py                  # POST /api/upload
â”‚  â”‚  â””â”€ query.py                   # POST /api/query (use_rag flag)
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ ingest_service.py          # PDF/image â†’ text â†’ chunks â†’ ChromaDB
â”‚  â”‚  â””â”€ rag_service.py             # Hybrid: general vs. RAG response
â”‚  â””â”€ utils/
â”‚     â”œâ”€ settings.py                # Config from .env
â”‚     â”œâ”€ logging_utils.py           # JSON-line query logging
â”‚     â””â”€ metrics_report.py          # CLI latency & precision metrics
â”œâ”€ demo_files/
â”‚  â””â”€ sample_manual.pdf
â””â”€ README.md
```

## Quick start

### 1. Backend

```bash
cd backend
python -m venv .venv

# Windows
.venv\Scripts\activate
# macOS / Linux
source .venv/bin/activate

pip install -r requirements.txt
uvicorn main:app --reload --port 8000
```

> **Tesseract OCR** (required for image upload):
> - Windows: download installer from https://github.com/UB-Mannheim/tesseract/wiki and add to PATH.
> - macOS: `brew install tesseract`
> - Linux: `sudo apt install tesseract-ocr`

### 2. Frontend

```bash
cd frontend
npm install
npm run dev
```

Open **http://localhost:3000**. The frontend talks to `http://localhost:8000` by default. Override:

```bash
NEXT_PUBLIC_API_URL=http://your-host:8000 npm run dev
```

## Hybrid RAG mode â€” how it works

| Scenario | Toggle state | Chat behavior |
| --- | --- | --- |
| No file uploaded | Disabled (OFF) | General AI chat (pure Gemini) |
| File uploaded | Auto-ON | RAG mode â€” answers reference the document |
| User flips toggle OFF | OFF | General AI chat (document stays indexed) |
| User flips toggle ON | ON | RAG mode resumes |

The backend respects the `use_rag` boolean in the `/api/query` request body:

```
IF use_rag == true AND file_id exists:
    â†’ retrieve chunks from ChromaDB â†’ generate grounded answer
ELSE:
    â†’ generate general response (no retrieval)
```

## API reference

| Method | Endpoint | Description |
| --- | --- | --- |
| GET | /api/health | `{"status": "ok"}` |
| POST | /api/upload | Multipart file â†’ ingest + index â†’ metadata |
| POST | /api/query | `{file_id, query, type, use_rag}` â†’ answer |

## Environment variables (backend/.env)

| Variable | Default | Description |
| --- | --- | --- |
| `GEMINI_API_KEY` | â€” | Google AI API key (required) |
| `GEMINI_MODEL` | `gemini-2.5-flash` | Model for generation |
| `USE_LOCAL_LLM` | `false` | Set `true` to use a local model instead |
| `CHUNK_SIZE` | `500` | Characters per chunk |
| `CHUNK_OVERLAP` | `100` | Overlap between chunks |
| `RAG_SIMILARITY_THRESHOLD` | `0.35` | Below this, fallback to general chat |

### Switching to a local LLM

Set `USE_LOCAL_LLM=true` in `.env` and replace the `generate_from_prompt()` function body in `backend/services/rag_service.py` with your local inference call (e.g., llama-cpp-python, vLLM, Ollama). Note: local models like LLaMA 7B require ~8 GB VRAM.

## Logging & Metrics

Every query is logged to `backend/logs/queries.jsonl`:

```json
{"timestamp": "...", "query": "...", "mode": "rag", "file_id": "doc-abc123",
 "use_rag": true, "retrieved_chunk_ids": ["..."], "retrieval_scores": [0.82],
 "retrieval_latency_ms": 120.5, "generation_latency_ms": 890.3,
 "total_latency_ms": 1015.2, "success": true, "error": null}
```

Run the metrics report:

```bash
cd backend
python -m utils.metrics_report
python -m utils.metrics_report --ground-truth ground_truth.json
```

## License

MIT

---

## ğŸš€ Deployment Guide

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      API calls      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Vercel        â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º â”‚   Render        â”‚
â”‚   (Frontend)    â”‚                     â”‚   (Backend)     â”‚
â”‚   Next.js       â”‚ â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚   FastAPI       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      JSON responses â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Step 1: Deploy Backend to Render

1. Go to [Render Dashboard](https://dashboard.render.com/)
2. Click **"New +"** â†’ **"Web Service"**
3. Connect your GitHub repo: `https://github.com/Otachiking/Chatbot-RAG`
4. Configure:
   - **Name**: `rag-chatbot-backend`
   - **Root Directory**: `backend`
   - **Runtime**: `Docker`
   - **Region**: Choose closest to your users
   - **Instance Type**: `Free` (or `Starter` for production)
5. Add **Environment Variables**:
   - `GEMINI_API_KEY` = your Google AI API key
   - `GEMINI_MODEL` = `gemini-2.5-flash`
6. (Optional) Add **Disk** for persistent ChromaDB:
   - Name: `chroma-data`
   - Mount Path: `/app/chroma_data`
   - Size: `1 GB`
7. Click **"Create Web Service"**
8. Wait for deployment â†’ Copy your backend URL (e.g., `https://rag-chatbot-backend.onrender.com`)

### Step 2: Deploy Frontend to Vercel

1. Go to [Vercel Dashboard](https://vercel.com/dashboard)
2. Click **"Add New..."** â†’ **"Project"**
3. Import your GitHub repo: `https://github.com/Otachiking/Chatbot-RAG`
4. Configure:
   - **Framework Preset**: `Next.js`
   - **Root Directory**: `frontend` â† **IMPORTANT!**
5. Add **Environment Variables**:
   - `NEXT_PUBLIC_API_URL` = `https://rag-chatbot-backend.onrender.com` (your Render URL)
6. Click **"Deploy"**

### Done! ğŸ‰

Your app is now live:
- **Frontend**: `https://your-project.vercel.app`
- **Backend**: `https://rag-chatbot-backend.onrender.com`

### Environment Variables Summary

| Service | Variable | Value |
|---------|----------|-------|
| Render (Backend) | `GEMINI_API_KEY` | Your Google AI API key |
| Render (Backend) | `GEMINI_MODEL` | `gemini-2.5-flash` |
| Vercel (Frontend) | `NEXT_PUBLIC_API_URL` | Your Render backend URL |

### Troubleshooting

| Issue | Solution |
|-------|----------|
| CORS error | Backend already allows all origins (`*`). Check if backend URL is correct. |
| 500 on upload | Check Render logs. Ensure `GEMINI_API_KEY` is set. |
| Cold start delay | Free tier sleeps after 15 min. First request takes ~30s. Upgrade to Starter. |
| ChromaDB data lost | Add a Render Disk (see Step 1.6) |

---

Made with â¤ï¸ by [Muhammad Iqbal Rasyid](https://portfolio-otachiking.vercel.app/)
